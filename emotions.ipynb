{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bfa430-3d48-4393-831b-b69d824b9158",
   "metadata": {},
   "source": [
    "<h1>Image Emotion Recognition Model By VGG16</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef62291-da3c-4763-af39-b960b11b5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import vgg16\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50247198-afee-4904-9e85-1645bedacce5",
   "metadata": {},
   "source": [
    "<h1>Initialising Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97d21dc-ce77-4fa2-8f56-c91142e08f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "IMG_WIDTH = 48\n",
    "IMG_HEIGHT = 48\n",
    "NUM_CLS = 7\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc694e3-b53c-4147-9045-a9d57533d2c6",
   "metadata": {},
   "source": [
    "<h1>Loading and Processing of Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309d9336-320b-4831-a26d-aa2b27fba5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for image_file in os.listdir(label_dir):\n",
    "                image_path = os.path.join(label_dir, image_file)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                image = image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(np.unique(labels))}\n",
    "    labels = [label_map[label] for label in labels]\n",
    "\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dfaa5-0e51-4c83-98c3-7bf3dc73ce44",
   "metadata": {},
   "source": [
    "<h1>Checking of Validity of Command Line Argument</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba3dd21-6b1f-4c13-92c6-9821761712da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) not in [2, 3]:\n",
    "    sys.exit(\"Usage: python traffic.py data_directory [model.h5]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fce6e-caed-44be-9aa0-fedc863db1f5",
   "metadata": {},
   "source": [
    "<h1>Separation of Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde33e4-86db-49ba-852b-a5b20e3dafa7",
   "metadata": {},
   "source": [
    "<h1>Early Stopping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318227d1-0133-4d75-b36a-1092d3429ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca43ac1-5cd5-45cb-9782-7ec1bedc239b",
   "metadata": {},
   "source": [
    "<h1>Preparation and Splitting of Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0c3ae-3bd5-420e-b5be-7d8f06ec3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f4c73-152c-4596-86c9-95713b29c092",
   "metadata": {},
   "source": [
    "<h1>Loading of pre-trained VGG16 Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9a5dc-23f1-437a-88a0-ace790034907",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a162e-906c-47f2-a445-adfb6f49a999",
   "metadata": {},
   "source": [
    "<h1>Model Structure</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ab212-f60d-4c5a-8652-b6106b82a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    vgg,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLS, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943f9e5-ca22-4470-9587-f362fc0b988b",
   "metadata": {},
   "source": [
    "<h1>Evaluation of Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537a6fe-ad6f-4539-957f-3e13c8019c76",
   "metadata": {},
   "source": [
    "<h2>Overall Accuracy: 87.5%</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a1e71-b84a-4aac-b47e-a78c154d5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c0c2a-d069-4457-b58e-60a66809fe36",
   "metadata": {},
   "source": [
    "13/13 - 1s - 73ms/step - accuracy: 0.8753 - loss: 0.4263"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937a380-08fd-48a7-8497-8172de273e01",
   "metadata": {},
   "source": [
    "<h1>Training Process</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0373ef-14e3-4473-abb9-d37e431327f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - accuracy: 0.4228 - loss: 1.8960\n",
    "Epoch 1: val_accuracy improved from -inf to 0.63359, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 150ms/step - accuracy: 0.4320 - loss: 1.8580 - val_accuracy: 0.6336 - val_loss: 1.4674\n",
    "Epoch 2/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - accuracy: 0.7275 - loss: 0.7745 \n",
    "Epoch 2: val_accuracy did not improve from 0.63359\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 134ms/step - accuracy: 0.7278 - loss: 0.7750 - val_accuracy: 0.6336 - val_loss: 1.2899\n",
    "Epoch 3/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - accuracy: 0.8103 - loss: 0.5689\n",
    "Epoch 3: val_accuracy improved from 0.63359 to 0.68193, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 138ms/step - accuracy: 0.8096 - loss: 0.5674 - val_accuracy: 0.6819 - val_loss: 1.1869\n",
    "Epoch 4/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - accuracy: 0.8307 - loss: 0.4762\n",
    "Epoch 4: val_accuracy improved from 0.68193 to 0.74046, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 137ms/step - accuracy: 0.8312 - loss: 0.4753 - val_accuracy: 0.7405 - val_loss: 1.0738\n",
    "Epoch 5/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - accuracy: 0.8761 - loss: 0.3989\n",
    "Epoch 5: val_accuracy improved from 0.74046 to 0.75573, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 153ms/step - accuracy: 0.8763 - loss: 0.3959 - val_accuracy: 0.7557 - val_loss: 0.9900\n",
    "Epoch 6/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - accuracy: 0.8803 - loss: 0.3247 \n",
    "Epoch 6: val_accuracy improved from 0.75573 to 0.78372, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 173ms/step - accuracy: 0.8809 - loss: 0.3229 - val_accuracy: 0.7837 - val_loss: 0.9292\n",
    "Epoch 7/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.8765 - loss: 0.3316 \n",
    "Epoch 7: val_accuracy improved from 0.78372 to 0.80662, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 171ms/step - accuracy: 0.8761 - loss: 0.3338 - val_accuracy: 0.8066 - val_loss: 0.8623\n",
    "Epoch 8/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - accuracy: 0.9027 - loss: 0.2851 \n",
    "Epoch 8: val_accuracy did not improve from 0.80662\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 152ms/step - accuracy: 0.9027 - loss: 0.2873 - val_accuracy: 0.7888 - val_loss: 0.8107\n",
    "Epoch 9/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - accuracy: 0.9283 - loss: 0.2432\n",
    "Epoch 9: val_accuracy did not improve from 0.80662\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 166ms/step - accuracy: 0.9278 - loss: 0.2444 - val_accuracy: 0.7863 - val_loss: 0.7751\n",
    "Epoch 10/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - accuracy: 0.9080 - loss: 0.2911 \n",
    "Epoch 10: val_accuracy did not improve from 0.80662\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 144ms/step - accuracy: 0.9087 - loss: 0.2894 - val_accuracy: 0.7710 - val_loss: 0.7209\n",
    "Epoch 11/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - accuracy: 0.9327 - loss: 0.2089\n",
    "Epoch 11: val_accuracy did not improve from 0.80662\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 137ms/step - accuracy: 0.9338 - loss: 0.2070 - val_accuracy: 0.7786 - val_loss: 0.6597\n",
    "Epoch 12/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - accuracy: 0.9474 - loss: 0.1766 \n",
    "Epoch 12: val_accuracy improved from 0.80662 to 0.83206, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 152ms/step - accuracy: 0.9470 - loss: 0.1756 - val_accuracy: 0.8321 - val_loss: 0.6335\n",
    "Epoch 13/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - accuracy: 0.9729 - loss: 0.1172 \n",
    "Epoch 13: val_accuracy did not improve from 0.83206\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 2s 130ms/step - accuracy: 0.9726 - loss: 0.1179 - val_accuracy: 0.8092 - val_loss: 0.5734\n",
    "Epoch 14/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - accuracy: 0.9728 - loss: 0.1067\n",
    "Epoch 14: val_accuracy did not improve from 0.83206\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 138ms/step - accuracy: 0.9728 - loss: 0.1072 - val_accuracy: 0.8066 - val_loss: 0.5742\n",
    "Epoch 15/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - accuracy: 0.9783 - loss: 0.1198 \n",
    "Epoch 15: val_accuracy improved from 0.83206 to 0.83969, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 149ms/step - accuracy: 0.9768 - loss: 0.1218 - val_accuracy: 0.8397 - val_loss: 0.5235\n",
    "Epoch 16/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - accuracy: 0.9663 - loss: 0.1205\n",
    "Epoch 16: val_accuracy improved from 0.83969 to 0.85496, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 157ms/step - accuracy: 0.9660 - loss: 0.1235 - val_accuracy: 0.8550 - val_loss: 0.5168\n",
    "Epoch 17/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - accuracy: 0.9704 - loss: 0.1261 \n",
    "Epoch 17: val_accuracy did not improve from 0.85496\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 154ms/step - accuracy: 0.9681 - loss: 0.1309 - val_accuracy: 0.8397 - val_loss: 0.5693\n",
    "Epoch 18/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - accuracy: 0.9484 - loss: 0.1455 \n",
    "Epoch 18: val_accuracy improved from 0.85496 to 0.87532, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 150ms/step - accuracy: 0.9482 - loss: 0.1459 - val_accuracy: 0.8753 - val_loss: 0.4263\n",
    "Epoch 19/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - accuracy: 0.9752 - loss: 0.0872 \n",
    "Epoch 19: val_accuracy improved from 0.87532 to 0.88295, saving model to best_model.keras\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 152ms/step - accuracy: 0.9745 - loss: 0.0904 - val_accuracy: 0.8830 - val_loss: 0.4389\n",
    "Epoch 20/20\n",
    "18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - accuracy: 0.9681 - loss: 0.1017 \n",
    "Epoch 20: val_accuracy did not improve from 0.88295\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 136ms/step - accuracy: 0.9673 - loss: 0.1055 - val_accuracy: 0.8702 - val_loss: 0.4602"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d5009-5c31-4207-b197-8aea3bc25df3",
   "metadata": {},
   "source": [
    "<h1>Model Summary</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33dca97-4486-4e72-8f27-4649dba10eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c1f33-0059-483b-bee2-4fef65ff77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model: \"sequential\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
    "│ vgg16 (Functional)                   │ (None, 1, 1, 512)           │      14,714,688 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ flatten (Flatten)                    │ (None, 512)                 │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense (Dense)                        │ (None, 1024)                │         525,312 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ batch_normalization                  │ (None, 1024)                │           4,096 │\n",
    "│ (BatchNormalization)                 │                             │                 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dropout (Dropout)                    │ (None, 1024)                │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_1 (Dense)                      │ (None, 7)                   │           7,175 │\n",
    "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
    " Total params: 16,320,343 (62.26 MB)\n",
    " Trainable params: 534,535 (2.04 MB)\n",
    " Non-trainable params: 14,716,736 (56.14 MB)\n",
    " Optimizer params: 1,069,072 (4.08 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb35e91-ef2a-45f9-b81c-b4e401829ef4",
   "metadata": {},
   "source": [
    "<h1>Plotting of Validation and Training Loss Values</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b9332de-5671-4fad-8211-e212501b7e6f",
   "metadata": {},
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce79a2d9-cefc-4c42-b114-f451f214910f",
   "metadata": {},
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb68887-b080-44a6-90d5-0e4d2b609a68",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wzngekj/Image-Emotion-Model/ab910ad8765ed49a37fe0c5cf07079550fda9453/Loss.png\" alt=\"Accuracy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8d60f-992e-43b7-9d6c-4dab0d5758b5",
   "metadata": {},
   "source": [
    "<h1>Plotting of Validation and Training Accuracy Values</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2de0b0c8-75c1-40e3-a777-7da8343bebcf",
   "metadata": {},
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15e042f0-45bb-4312-ac6e-0146a346e81a",
   "metadata": {},
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee63ce7-5637-42ae-8197-69b24f6fafc0",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wzngekj/Image-Emotion-Model/ab910ad8765ed49a37fe0c5cf07079550fda9453/Accuracy.png\" alt=\"Accuracy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
