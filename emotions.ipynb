{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bfa430-3d48-4393-831b-b69d824b9158",
   "metadata": {},
   "source": [
    "<h1>Image Emotion Recognition Model By VGG16</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef62291-da3c-4763-af39-b960b11b5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications import vgg16\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50247198-afee-4904-9e85-1645bedacce5",
   "metadata": {},
   "source": [
    "<h1>Initialising Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97d21dc-ce77-4fa2-8f56-c91142e08f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "IMG_WIDTH = 48\n",
    "IMG_HEIGHT = 48\n",
    "NUM_CLS = 7\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc694e3-b53c-4147-9045-a9d57533d2c6",
   "metadata": {},
   "source": [
    "<h1>Loading and Processing of Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309d9336-320b-4831-a26d-aa2b27fba5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for image_file in os.listdir(label_dir):\n",
    "                image_path = os.path.join(label_dir, image_file)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                image = image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(np.unique(labels))}\n",
    "    labels = [label_map[label] for label in labels]\n",
    "\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dfaa5-0e51-4c83-98c3-7bf3dc73ce44",
   "metadata": {},
   "source": [
    "<h1>Checking of Validity of Command Line Argument</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba3dd21-6b1f-4c13-92c6-9821761712da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) not in [2, 3]:\n",
    "    sys.exit(\"Usage: python traffic.py data_directory [model.h5]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fce6e-caed-44be-9aa0-fedc863db1f5",
   "metadata": {},
   "source": [
    "<h1>Separation of Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde33e4-86db-49ba-852b-a5b20e3dafa7",
   "metadata": {},
   "source": [
    "<h1>Early Stopping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318227d1-0133-4d75-b36a-1092d3429ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca43ac1-5cd5-45cb-9782-7ec1bedc239b",
   "metadata": {},
   "source": [
    "<h1>Preparation and Splitting of Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0c3ae-3bd5-420e-b5be-7d8f06ec3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f4c73-152c-4596-86c9-95713b29c092",
   "metadata": {},
   "source": [
    "<h1>Loading of pre-trained VGG16 Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9a5dc-23f1-437a-88a0-ace790034907",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a162e-906c-47f2-a445-adfb6f49a999",
   "metadata": {},
   "source": [
    "<h1>Model Structure</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ab212-f60d-4c5a-8652-b6106b82a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    vgg,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLS, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943f9e5-ca22-4470-9587-f362fc0b988b",
   "metadata": {},
   "source": [
    "<h1>Evaluation of Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537a6fe-ad6f-4539-957f-3e13c8019c76",
   "metadata": {},
   "source": [
    "<h2>Overall Accuracy: 89.9%</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a1e71-b84a-4aac-b47e-a78c154d5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c0c2a-d069-4457-b58e-60a66809fe36",
   "metadata": {},
   "source": [
    "13/13 - 1s - 111ms/step - accuracy: 0.8982 - loss: 0.3269"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937a380-08fd-48a7-8497-8172de273e01",
   "metadata": {},
   "source": [
    "<h1>Training Process</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0373ef-14e3-4473-abb9-d37e431327f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 5s 180ms/step - accuracy: 0.3578 - loss: 2.0278 - val_accuracy: 0.4529 - val_loss: 1.4921\n",
    "Epoch 2/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 192ms/step - accuracy: 0.7421 - loss: 0.7711 - val_accuracy: 0.5598 - val_loss: 1.3423\n",
    "Epoch 3/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 191ms/step - accuracy: 0.8007 - loss: 0.5705 - val_accuracy: 0.7252 - val_loss: 1.1897\n",
    "Epoch 4/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 189ms/step - accuracy: 0.8615 - loss: 0.4192 - val_accuracy: 0.7379 - val_loss: 1.0806\n",
    "Epoch 5/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 193ms/step - accuracy: 0.8674 - loss: 0.3600 - val_accuracy: 0.7354 - val_loss: 1.0001\n",
    "Epoch 6/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 183ms/step - accuracy: 0.9170 - loss: 0.2923 - val_accuracy: 0.7532 - val_loss: 0.9280\n",
    "Epoch 7/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 188ms/step - accuracy: 0.8923 - loss: 0.3235 - val_accuracy: 0.7812 - val_loss: 0.8609\n",
    "Epoch 8/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 188ms/step - accuracy: 0.9261 - loss: 0.2716 - val_accuracy: 0.7761 - val_loss: 0.8086\n",
    "Epoch 9/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 191ms/step - accuracy: 0.9156 - loss: 0.2333 - val_accuracy: 0.8117 - val_loss: 0.7093\n",
    "Epoch 10/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 190ms/step - accuracy: 0.9119 - loss: 0.2361 - val_accuracy: 0.8244 - val_loss: 0.6627\n",
    "Epoch 11/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 191ms/step - accuracy: 0.9452 - loss: 0.1998 - val_accuracy: 0.8346 - val_loss: 0.6285\n",
    "Epoch 12/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 188ms/step - accuracy: 0.9436 - loss: 0.2000 - val_accuracy: 0.7735 - val_loss: 0.6018\n",
    "Epoch 13/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 192ms/step - accuracy: 0.9499 - loss: 0.1622 - val_accuracy: 0.8117 - val_loss: 0.5580\n",
    "Epoch 14/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 192ms/step - accuracy: 0.9636 - loss: 0.1488 - val_accuracy: 0.8219 - val_loss: 0.5349\n",
    "Epoch 15/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 191ms/step - accuracy: 0.9617 - loss: 0.1589 - val_accuracy: 0.8321 - val_loss: 0.5439\n",
    "Epoch 16/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 194ms/step - accuracy: 0.9511 - loss: 0.1422 - val_accuracy: 0.8168 - val_loss: 0.5174\n",
    "Epoch 17/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 190ms/step - accuracy: 0.9684 - loss: 0.1394 - val_accuracy: 0.8702 - val_loss: 0.3847\n",
    "Epoch 18/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 192ms/step - accuracy: 0.9679 - loss: 0.1254 - val_accuracy: 0.8880 - val_loss: 0.3628\n",
    "Epoch 19/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 192ms/step - accuracy: 0.9606 - loss: 0.1233 - val_accuracy: 0.8982 - val_loss: 0.3269\n",
    "Epoch 20/20\n",
    "19/19 ━━━━━━━━━━━━━━━━━━━━ 4s 194ms/step - accuracy: 0.9716 - loss: 0.0954 - val_accuracy: 0.9135 - val_loss: 0.3321"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d5009-5c31-4207-b197-8aea3bc25df3",
   "metadata": {},
   "source": [
    "<h1>Model Summary</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33dca97-4486-4e72-8f27-4649dba10eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c1f33-0059-483b-bee2-4fef65ff77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model: \"sequential\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
    "│ vgg16 (Functional)                   │ (None, 1, 1, 512)           │      14,714,688 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ flatten (Flatten)                    │ (None, 512)                 │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense (Dense)                        │ (None, 1024)                │         525,312 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ batch_normalization                  │ (None, 1024)                │           4,096 │\n",
    "│ (BatchNormalization)                 │                             │                 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dropout (Dropout)                    │ (None, 1024)                │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_1 (Dense)                      │ (None, 7)                   │           7,175 │\n",
    "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
    " Total params: 16,320,343 (62.26 MB)\n",
    " Trainable params: 534,535 (2.04 MB)\n",
    " Non-trainable params: 14,716,736 (56.14 MB)\n",
    " Optimizer params: 1,069,072 (4.08 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb35e91-ef2a-45f9-b81c-b4e401829ef4",
   "metadata": {},
   "source": [
    "<h1>Plotting of Validation and Training Loss Values</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b9332de-5671-4fad-8211-e212501b7e6f",
   "metadata": {},
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb68887-b080-44a6-90d5-0e4d2b609a68",
   "metadata": {},
   "source": [
    "![Image Title](Loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8d60f-992e-43b7-9d6c-4dab0d5758b5",
   "metadata": {},
   "source": [
    "<h1>Plotting of Validation and Training Accuracy Values</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2de0b0c8-75c1-40e3-a777-7da8343bebcf",
   "metadata": {},
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d71a86-f1cd-4d73-8620-c2aeb8b997a9",
   "metadata": {},
   "source": [
    "![Image Title](Accuracy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
